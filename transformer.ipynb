{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPe66fj3e6zroa9OObZyUiQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RainingCodes/Simple_Transformer/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as text  # 한국어 처리를 위해 필요\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "boga7BQr7xoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenizer 정의 (Word-based에서 Vectorization 사용 / Tokinizer : 단순히 \"문장을 숫자로 변환\"하는 것이 아니라, 적절한 방식으로 나누고(토큰화) 변환하는 과정)\n",
        "vectorizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=10000, output_mode=\"int\", output_sequence_length=5\n",
        ")\n"
      ],
      "metadata": {
        "id": "NmSQvTpo71JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 학습 데이터 (한국어 → 영어 번역 예제)\n",
        "train_sentences = [\n",
        "    \"나는 학생입니다\",  # Input (한국어)\n",
        "    \"그녀는 선생님입니다\",\n",
        "    \"오늘 날씨가 좋네요\"\n",
        "]\n",
        "target_sentences = [\n",
        "    \"I am a student\",  # Output (영어)\n",
        "    \"She is a teacher\",\n",
        "    \"Today's weather is good\"\n",
        "]"
      ],
      "metadata": {
        "id": "x__2ERKG8EOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer 학습 (텍스트를 숫자로 변환할 수 있도록 어휘 사전 구축)\n",
        "vectorizer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "_R-_wWfI8HmX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Self-Attention 블록 정의\n",
        "class SimpleSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim):\n",
        "        super(SimpleSelfAttention, self).__init__()\n",
        "        self.W_q = tf.keras.layers.Dense(dim)  # Query 변환 행렬\n",
        "        self.W_k = tf.keras.layers.Dense(dim)  # Key 변환 행렬\n",
        "        self.W_v = tf.keras.layers.Dense(dim)  # Value 변환 행렬\n",
        "\n",
        "    def call(self, x):\n",
        "        Q = self.W_q(x)  # Query 행렬 생성\n",
        "        K = self.W_k(x)  # Key 행렬 생성\n",
        "        V = self.W_v(x)  # Value 행렬 생성\n",
        "\n",
        "        # Self-Attention의 핵심 연산: 가중치 계산\n",
        "        attention_scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(tf.cast(K.shape[-1], tf.float32))\n",
        "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
        "\n",
        "        # 중요도가 높은 단어일수록 더 반영됨\n",
        "        return tf.matmul(attention_weights, V)\n"
      ],
      "metadata": {
        "id": "tqQrT41I8Iav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Transformer 모델 정의 (Embedding + Self-Attention)\n",
        "class SimpleTransformer(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, dim=16):\n",
        "        super(SimpleTransformer, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, dim)  # 단어를 의미 공간에 매핑\n",
        "        self.self_attention = SimpleSelfAttention(dim)  # Self-Attention 적용\n",
        "        self.dense = tf.keras.layers.Dense(10, activation=\"softmax\")  # 최종 출력\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)  # 문장을 벡터 공간으로 변환\n",
        "        x = self.self_attention(x)  # Self-Attention 적용하여 관계 파악\n",
        "        return self.dense(x)  # 변환된 결과"
      ],
      "metadata": {
        "id": "MmwKe0hc8KCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 모델 생성 및 학습\n",
        "vocab_size = 10000  # 어휘 사전 크기\n",
        "model = SimpleTransformer(vocab_size)\n"
      ],
      "metadata": {
        "id": "CM2cQamS941p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터 준비 (문장을 벡터로 변환)\n",
        "X_train = vectorizer(train_sentences)\n",
        "Y_train = vectorizer(target_sentences)\n"
      ],
      "metadata": {
        "id": "tEumo90C97-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
        "model.fit(X_train, Y_train, epochs=10)"
      ],
      "metadata": {
        "id": "yLvh-nhlQ7_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 추론 (Inference) 테스트\n",
        "test_sentence = \"나는 학생입니다\"\n",
        "test_vector = vectorizer([test_sentence])  # 변환된 숫자 입력\n",
        "output = model(test_vector)"
      ],
      "metadata": {
        "id": "TOdE-_wdQ-Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 결과 해석\n",
        "print(\"입력 문장:\", test_sentence)\n",
        "print(\"출력 벡터:\", output.numpy())  # 예측된 벡터 출력"
      ],
      "metadata": {
        "id": "HfbpAGc8RChv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}